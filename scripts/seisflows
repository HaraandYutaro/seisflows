#!/usr/bin/env python
"""
This script controls Seisflows.

This script needs to be run in a directory containing `parameters.yaml` which
should contain user defined paths and parameters to be used in the workflow
"""
import os
import sys
import argparse
import numpy as np
from glob import glob

from seisflows.tools import unix, tools
from seisflows.config import config, tilde_expand, Dict, names
from seisflows.tools.tools import loadyaml


def get_args():
    """
    Get User defined arguments, or assign defaults

    :rtype: argparse.ArgumentParser()
    :return: User defined or default arguments
    """
    parser = argparse.ArgumentParser()

    # Positional arguments
    parser.add_argument("run", type=str, nargs="?",
                        choices=["submit", "clean", "resume", "debug",
                                 "restart"],
                        help="task for Seisflows to perform")

    # Optional parameters
    parser.add_argument("-w", "--workdir", nargs="?", default=os.getcwd())
    parser.add_argument("-p", "--parameter_file", nargs="?",
                        default="parameters.yaml")
    parser.add_argument("-c", "--continuous", nargs="?", default=True)

    return parser.parse_args()


def parse(parameters):
    """
    Seisflows was written in such a way that excluding parameters from the 
    parameter file would enforce default values. This becomes confusing however 
    because then the User needs to look at the definitions in the check()
    functions of each module to determine which parameters they want to exclude.

    Rather than rewrite this system, we allow the User to set parameters to 
    None, null, '', etc. This function sanitizes those inputs before handing 
    them over to Seisflows. This allows for full parameter files that include 
    all options that also works with the machinery of Seisflows

    This function also enforces a few string formats 

    :type parameters: dict
    :param parameters: dict of parameters to parse
    :rtype: dict
    :return: parameters that have been sanitized of all null values
    """
    # Copy the dictionary to get around deleting keys while iterating
    parsed_parameters = dict(parameters)
    for key, item in parameters.items():
        # Search for all None and "" items, ignore bools, 0's etc.
        if not item and isinstance(item, (type(None), str)):
            del parsed_parameters[key]
        elif key == "LINESEARCH":
            parsed_parameters[key] = item.capitalize()
        elif key == "RESUME_FROM":
            check = input(f"\nPAR.{key} is set to {item}, proceed? (y/[n]): ")
            if check != "y":
                sys.exit(-1)

    return parsed_parameters


def setup():
    """
    Common setup for multiple functions
    """
    args = get_args()

    # Check if the filepaths exist
    if not os.path.exists(args.parameter_file):
        raise Exception(f"Parameter file not found: {args.parameter_file}")

    # Register parameters
    parameters = loadyaml(args.parameter_file)
    parameters = parse(parameters)
    sys.modules['seisflows_parameters'] = Dict(parameters)

    # Register paths, expand to relative paths to absolute
    paths = tilde_expand(parameters['PATHS'])
    for key, path in paths.items():
        paths[key] = os.path.abspath(path)
    sys.modules['seisflows_paths'] = Dict(paths)

    return args, paths, parameters


def submit():
    """
    Submit the workflow for the first time
    """
    args, paths, parameters = setup()

    # Check that paths exist
    paths_dont_exist = []
    for key, path in paths.items():
        if (key in ["OUTPUT", "SCRATCH", "PYATOA_IO"]) or (path == ""):
            continue
        if not os.path.exists(path):
            paths_dont_exist.append(path)
    if paths_dont_exist:
        print("\nThe following paths do not exist:\n")
        for path_ in paths_dont_exist:
            print(f"\t{path_}")
        print("\n")
        sys.exit()

    # If paths exist, pass to modules
    sys.modules["seisflows_paths"] = Dict(paths)
    unix.mkdir(args.workdir)
    unix.cd(args.workdir)

    # Submit workflow
    config()
    workflow = sys.modules["seisflows_workflow"]
    system = sys.modules["seisflows_system"]

    system.submit(workflow)


def resume(debug=False):
    """
    Resume a previously started workflow

    :type debug: bool
    :param debug: Do not submit the workflow, but rather open a debugger so the
        User can search and step through the workflow
    """
    args, paths, parameters = setup()

    # Work directory should already be created
    unix.cd(args.workdir)

    # Reload objects from Pickle files
    for name in names:
        fullfile = os.path.join(args.workdir, "output", f"seisflows_{name}.p")
        sys.modules[f"seisflows_{name}"] = tools.loadobj(fullfile)

    # Check parameters
    for name in names:
        sys.modules[f"seisflows_{name}"].check()

    workflow = sys.modules["seisflows_workflow"]
    system = sys.modules["seisflows_system"]

    if debug:
        # Prematurely distribute modules for easier debugging
        PATH = sys.modules["seisflows_paths"]
        PAR = sys.modules["seisflows_parameters"]

        solver = sys.modules["seisflows_solver"]
        optimize = sys.modules["seisflows_optimize"]
        postprocess = sys.modules["seisflows_postprocess"]
        
        # Open the debugger
        import ipdb; ipdb.set_trace()
    else:
        system.submit(workflow)


def clean(workdir):
    """
    Clean the working directory

    :type workdir: str
    :param workdir: working directory to clean
    """
    check = input("\nThis will remove all workflow objects, leaving only the "
                  "parameter file.\nAre you sure you want to clean? (y/[n]): ")
    if check == "y":
        for fid in glob(os.path.join(workdir, "output*")):
            unix.rm(fid)
        for fid in glob(os.path.join(workdir, "*log*")):
            unix.rm(fid)
        unix.rm(os.path.join(workdir, "scratch"))
        unix.rm(os.path.join(workdir, "pyatoa.io"))


def convert_model(solver, path_output, model_fid):
    """
    convert model .npy to .bin files

    TODO:
        figure out where to put this in seisflows

    :type solver: module
    :param solver: seisflows.solver
    :type path_output: str
    :param path_output: path to the 'output' directory where models are saved
    :type model_fid: str
    :param model_fid: the model that should be converted
    """
    src = os.path.join(path_output, f"{model_fid}.npy")
    dst = os.path.join(path_output, model_fid)

    if not os.path.exists(src):
        print(f"{src} does not exist")
        return
    elif os.path.exists(dst):
        print(f"{src} already exists")
        return

    solver.save(solver.split(np.load(src)), dst)


def status(continuous):
    """
    Monitor the status of the workflow

    %i job_id-[start-end]
    %F job_id
    %K array_start-array_end
    %T extended job status e.g. RUNNING
    %L time left
    
    :type continuous: bool
    :param continous: keep an up to date status, rather than a one time print
    """
    from subprocess import check_output
    
    def chkout(arg):
        """small wrapper for check_output
        """
        return check_output(arg, shell=True).decode("utf-8").strip()

    args, paths, par = setup()
    
    if par["SYSTEM"] == "maui_lg":
        output_log = glob.glob(os.path.join(args.workdir, "output*.log"))[0]
        error_log = glob.glob(os.path.join(args.workdir, "error*.log"))[0]

        base = (f"squeue --noheader --clusters={par['MAIN_CLUSTER']} "
                f"--account={par['ACCOUNT']}")
   
        # Get the currently running job id
        job_id = chkout(" ".join([base, "--name={par['TITLE'] -o %F"]))
        job_id = job_id.split("\n")[0]
        
        # Determine if multiple jobs or single job running
        array_check = chkout(" ".join([base, "--name={par['TITLE'] -o %K"]))
        array_check = array_check.split("\n")[0]
        # Single job running
        if array_check == "N/A":
            status = chkout(" ".join([base, "--job={job_id} -o '%T,%L'"]))
            status = status.split("\n")[1]
            status, timeleft = status.split(",")
        # Multiple jobs running
        else:
            status = np.zeros(par['NTASK'])
    else:
        raise NotImplementedError(f"System {parameters['SYSTEM']} not "
                                  f"supported")


if __name__ == "__main__":
    args_ = get_args()
    if args_.run == "submit":
        submit()
    elif args_.run == "resume":
        resume(debug=False)
    elif args_.run == "debug":
        resume(debug=True)
    elif args_.run == "clean":
        clean(args_.workdir)
    elif args_.run == "restart":
        clean(args_.workdir)
        submit()
    elif args_.run == "status":
        status(continuous=args_.continous)
