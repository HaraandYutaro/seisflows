#!/usr/bin/env python3
"""
The simplest simulation workflow you can run is a large number of forward
simulations to generate synthetics from a velocity model. Therefore the
Forward class represents the BASE workflow. All other workflows will build off
of the scaffolding defined by the Forward class.
"""
import os
from time import asctime

from seisflows import logger
from seisflows.tools import msg, unix
from seisflows.core import Dict
from seisflows.config import import_seisflows


class Forward:
    """
    [workflow.forward] Run forward solver in parallel and optionally calculate
    data-synthetic misfit and adjoint sources.

    :type modules: list of module
    :param modules: instantiated SeisFlows modules which should have been
        generated by the function `seisflows.config.import_seisflows` with a
        parameter file generated by seisflows.configure
    :type data_case: str
    :param data_case: How to address 'data' in the workflow, available options:
        'data': real data will be provided by the user in
        `path_data/{source_name}` in the same format that the solver will
        produce synthetics (controlled by `solver.format`) OR
        synthetic': 'data' will be generated as synthetic seismograms using
        a target model provided in `path_model_true`. If None, workflow will
        not attempt to generate data.
    :type export_traces: bool
    :param export_traces: export all waveforms that are generated by the
        external solver to `path_output`. If False, solver traces stored in
        scratch may be discarded at any time in the workflow
    :type export_residuals: bool
    :param export_residuals: export all residuals (data-synthetic misfit) that
        are generated by the external solver to `path_output`. If False,
        residuals stored in scratch may be discarded at any time in the workflow
    :type workdir: str
    :param workdir: working directory in which to look for data and store
        results. Defaults to current working directory
    :type path_eval_grad: str
    :param path_eval_grad: scratch path to store files for gradient evaluation,
        including models, kernels, gradient and residuals.
    """
    def __init__(self, modules=None, data_case=None, export_traces=False,
                 export_residuals=False, workdir=os.getcwd(),
                 path_eval_grad=None, path_output=None, path_data=None,
                 path_state_file=None, path_model_init=None,
                 path_model_true=None, **kwargs):
        """Set default forward workflow parameters"""
        # Keep modules hidden so that seisflows configure doesnt count them
        # as 'parameters'
        self._modules = modules

        self.data_case = data_case
        self.export_traces = export_traces
        self.export_residuals = export_residuals

        self.path = Dict(
            workdir=workdir,
            scratch=os.path.join(workdir, "scratch"),
            eval_grad=path_eval_grad or
                      os.path.join(workdir, "scratch", "evalgrad"),
            output=path_output or os.path.join(workdir, "output"),
            state_file=path_state_file or
                       os.path.join(workdir, "statefile.txt"),
            data=path_data,
            model_init=path_model_init,
            model_true=path_model_true
        )

        self._required_modules = ["system", "solver"]
        self._acceptable_data_cases = ["data", "synthetic"]
        self._optional_modules = ["preprocess"]

        # Empty module variables that should be filled in by setup
        self.system = None
        self.solver = None
        self.preprocess = None

        # Read in any existing state file which keeps track of workflow tasks
        self._states = {}
        if os.path.exists(self.path.state_file):
            for line in open(self.path.state_file, "r").readlines():
                if line.startswith("#"):
                    continue
                key, val = line.strip().split(":")
                self._states[key] = val.strip()

    @property
    def task_list(self):
        """
        USER-DEFINED TASK LIST. This property defines a list of class methods
        that take NO INPUT and have NO RETURN STATEMENTS. This defines your
        linear workflow, i.e., these tasks are to be run in order from start to
        finish to complete a workflow.

        This excludes 'check' (which is run during 'import_seisflows') and
        'setup' which should be run separately

        .. note::
            For workflows that require an iterative approach (e.g. inversion),
            this task list will be looped over, so ensure that any setup and
            teardown tasks (run once per workflow, not once per iteration) are
            not included.

        :rtype: list
        :return: list of methods to call in order during a workflow
        """
        return [self.evaluate_initial_misfit]

    def check(self):
        """
        Check that workflow has required modules. Run their respective checks
        """
        # Check that required modules have been instantiated
        for req_mod in self._required_modules:
            assert(self._modules[req_mod]), (
                f"'{req_mod}' is a required module for workflow " 
                f"'{self.__class__.__name__}'"
            )
            # Make sure that the modules are actually instances (not e.g., str)
            assert(hasattr(self._modules[req_mod], "__class__")), \
                f"workflow attribute {req_mod} must be an instance"

            # Run check function of these modules
            self._modules[req_mod].check()

        # Tell the user whether optional modules are instantiated
        for opt_mod in self._optional_modules:
            if self._modules[opt_mod]:
                self._modules[opt_mod].check()
            else:
                logger.warning(f"optional module '{opt_mod}' has not been "
                               f"instantiated, some functionality of the "
                               f"'{self.__class__.__name__}' workflow may be "
                               f"skipped")

        if self.data_case is not None:
            assert(self.data_case.lower() in self._acceptable_data_cases), \
                f"`data_case` must be in {self._acceptable_data_cases}"
            if self.data_case.lower() == "data":
                assert(self.path.data is not None and
                       os.path.exists(self.path.data)), \
                    f"importing data with `data_case`=='import' requires " \
                    f"'path_data' to exist"
            elif self.data_case.lower() == "synthetic":
                assert(self.path.model_true is not None and
                       os.path.exists(self.path.model_true)), \
                    f"creating data with `data_case`=='create' requires " \
                    f"'path_model_true' to exist and point to a target model"
        else:
            logger.warning(f"`workflow.data_case` is None, SeisFlows will not "
                           f"be able to find data for data-synthetic comparison"
                           )

    def setup(self):
        """
        Makes required path structure for the workflow, runs setup functions
        for all the required modules of this workflow.
        """
        # Create the desired directory structure
        for path in self.path.values():
            if path is not None and not os.path.splitext(path)[-1]:
                unix.mkdir(path)

        # Run setup() for each of the required modules
        for req_mod in self._required_modules:
            logger.info(
                f"setup for module "
                f"'{req_mod}.{self._modules[req_mod].__class__.__name__}'"
            )
            self._modules[req_mod].setup()

        # Run setup() for each of the instantiated modules
        for opt_mod in self._optional_modules:
            if self._modules[opt_mod]:
                logger.info(
                    f"setup for module "
                    f"'{opt_mod}.{self._modules[opt_mod].__class__.__name__}'"
                )
                self._modules[opt_mod].setup()

        # Generate the state file to keep track of task completion
        if not os.path.exists(self.path.state_file):
            logger.info(f"generating SeisFlows state file")
            logger.debug(self.path.state_file)
            with open(self.path.state_file, "w") as f:
                f.write(f"# SeisFlows State File\n")
                f.write(f"# {asctime()}\n")
                f.write(f"# Acceptable states: 'completed', 'failed'\n")
                f.write(f"# =======================================\n")

        # Distribute modules to the class namespace. We don't do this at init
        # incase _modules was set as NoneType
        self.solver = self._modules.solver
        self.system = self._modules.system
        self.preprocess = self._modules.preprocess

    def checkpoint(self):
        """
        Saves active SeisFlows working state to disk as Pickle files such that
        the workflow can be resumed following a crash, pause or termination of
        workflow.
        """
        # Grab State file header values
        with open(self.path.state_file, "r") as f:
            lines = f.readlines()

        with open(self.path.state_file, "w") as f:
            # Rewrite header values
            for line in lines:
                if line.startswith("#"):
                    f.write(line)
            for key, val in self._states.items():
                f.write(f"{key}: {val}\n")

    def run(self):
        """
        Call the Task List in order to 'run' the workflow. Contains logic for
        to keep track of completed tasks and avoids re-running tasks that have
        previously been completed (e.g., if you are restarting your workflow)
        """
        for func in self.task_list:
            # Skip over functions which have already been completed
            if (func.__name__ in self._states.keys()) and (
                    self._states[func.__name__] == "completed"):
                logger.info(f"'{func.__name__}' has already been run, skipping")
                continue
            # Otherwise attempt to run functions that have failed or are
            # encountered for the first time
            else:
                try:
                    func()
                    self._states[func.__name__] = "completed"
                except Exception as e:
                    self._states[func.__name__] = "failed"
                    self.checkpoint()
                    raise

        self.checkpoint()

    def evaluate_initial_misfit(self):
        """
        System wrapper for 'evaluate_objective function' that passes in
        """
        logger.info(msg.mnr("EVALUATING MISFIT FOR INITIAL MODEL"))
        self.system.run([self.prepare_data_for_solver,
                         self.evaluate_objective_function],
                        path_model=self.path.model_init
                        )

    def prepare_data_for_solver(self, **kwargs):
        """
        Determines how to provide data to each of the solvers. Either by copying
        data in from a user-provided path, or generating synthetic 'data' using
        a target model.

        .. note ::
            Must be run by system.run() so that solvers are assigned individual
            task ids and working directories
        """
        logger.info(msg.sub("preparing data for solver"))

        if self.data_case == "data":
            logger.info(f"copying data from `path_data`")
            src = os.path.join(self.path.data, self.solver.source_name, "*")
            dst = os.path.join(self.solver.cwd, "traces", "obs", "")
            unix.cp(src, dst)
        elif self.data_case == "synthetic":
            # Figure out where to export waveform files to, if requested
            if self.export_traces:
                export_traces = os.path.join(self.path.output,
                                             self.solver.source_name, "obs")
            else:
                export_traces = False

            # Run the forward solver with target model and save traces the 'obs'
            logger.info(f"running forward simulation for "
                        f"{self.solver.source_name}")
            self.solver.import_model(path_model=self.path.model_true)
            self.solver.forward_simulation(
                save_traces=os.path.join(self.solver.cwd, "traces", "obs"),
                export_traces=export_traces
            )

    def evaluate_objective_function(self, path_model, **kwargs):
        """
        Performs forward simulation for a single given event. Also evaluates the
        objective function and writes residuals and adjoint sources for later
        tasks.

        .. note::
            if PAR.PREPROCESS == None, will not perform misfit quantification

        .. note::
            Must be run by system.run() so that solvers are assigned individual
            task ids/ working directories.
        """
        logger.info(f"running forward simulation with "
                    f"'{self.solver.__class__.__name__}'")

        # Figure out where to export waveform files to, if requested
        if self.export_traces:
            export_traces = os.path.join(self.path.output,
                                         self.solver.source_name, "syn")
        else:
            export_traces = False

        # Run the forward simulation with the given input model
        self.solver.import_model(path_model=path_model)
        self.solver.forward_simulation(
            save_traces=os.path.join(self.solver.cwd, "traces", "syn"),
            export_traces=export_traces
        )

        # (optional) Perform data-synthetic misfit quantification
        if self.preprocess:
            logger.info(f"quantifying misfit with "
                        f"'{self.preprocess.__class__.__name__}'")
            self.preprocess.quantify_misfit(
                observed=self.solver.data_filenames(choice="obs"),
                synthetics=self.solver.data_filenames(choice="syn"),
                save_adjsrcs=os.path.join(self.solver.cwd, "traces", "adj"),
                save_residuals=os.path.join(self.path.evalgrad, "residuals")
            )

    # def alsdkjfla:
    #     self._write_misfit(path=path, misfit_tag=misfit_tag)
    #
    # def _write_model(self, path, model_tag):
    #     """
    #     Writes model in format expected by solver
    #
    #     :type path: str
    #     :param path: path to write the model to
    #     :type model_tag: str
    #     :param model_tag: name of the model to be saved, usually tagged as 'm' with
    #         a suffix depending on where in the inversion we are. e.g., 'm_try'.
    #         Expected that these tags are defined in OPTIMIZE module
    #     """
    #     m = Model(path=os.path.join(self.path.OPTIMIZE, f"{model_tag}.npz"))
    #     m.write(path=os.path.join(path, "model"))
    #
    #     self.logger.debug(f"saving model '{model_tag}'")
    #
    # def _write_misfit(self, path, misfit_tag):
    #     """
    #     Writes misfit in format expected by nonlinear optimization library.
    #     Collects all misfit values within the given residuals directory and sums
    #     them in a manner chosen by the preprocess class.
    #
    #     :type path: str
    #     :param path: path to write the misfit to
    #     :type misfit_tag: str
    #     :param misfit_tag: name of the model to be saved, usually tagged as
    #         'f' with a suffix depending on where in the inversion we are.
    #         e.g., 'f_try'. Expected that these tags are defined in OPTIMIZE
    #         module
    #     """
    #     preprocess = self.module("preprocess")
    #     optimize = self.module("optimize")
    #
    #     self.logger.info("summing residuals with preprocess module")
    #     src = glob(os.path.join(path, "residuals", "*"))
    #     total_misfit = preprocess.sum_residuals(src)
    #
    #     self.logger.debug(f"saving misfit {total_misfit:.3E} to '{misfit_tag}'")
    #     optimize.save(misfit_tag, total_misfit)


if __name__ == "__main__":
    # Standard SeisFlows setup, makes modules global variables to the workflow
    pars, modules = import_seisflows()

    logger.info(msg.mjr("Starting forward simulation workflow"))

    workflow = Forward(modules, **pars)
    workflow.check()
    workflow.setup()
    workflow.run()

    logger.info(msg.mjr("Finished forward simulation workflow"))

